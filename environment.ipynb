{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "author": "mes (5-1) + (+1) = 0.84$ となる。",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "environment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ieyasu2017/Thermal/blob/master/environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SwhWK6kYYbs",
        "colab_type": "text"
      },
      "source": [
        "<h1>迷路環境での Markov Decision Process (MDP)</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBq0vBDkYYbu",
        "colab_type": "text"
      },
      "source": [
        "プログラムで実装している MDP を満たす環境\n",
        "\n",
        "<img src=\"https://ieyasu03.web.fc2.com/Deep_Learning/Fig18-1.jpg\">\n",
        "\n",
        "\n",
        "　エージェントは上下左右への移動が可能であり、状態は迷路の現在位置となる。青のセルに到達したらプラスの報酬（$+1$）でゴール、赤のセルに到達したらマイナスの報酬（$-1$）でゴールとなる。黒のセルは移動できないブロックとする。また、セルの移動ごとにマイナスの報酬（$-0.04$）を設定している。\n",
        " \n",
        "　方策　：　上下左右からランダムに行動を選択\n",
        " \n",
        "　遷移確率　：　選択された方向は $0.8$、その反対方向は $0$、その他の方向 $(1-0.8)/2=0.1$\n",
        " \n",
        "としている。\n",
        "\n",
        "　図の破線の経路は $5$ ステップで報酬を最大とする最適経路となっており、そのときの報酬は\n",
        " \n",
        " $$ \\mathrm{Reward} = -0.04 \\times (5-1) + (+1) = 0.84$$ となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3r_zjrCYYbu",
        "colab_type": "text"
      },
      "source": [
        "<h3>状態・行動・環境の設定</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "run_control": {
          "marked": false
        },
        "id": "JWdGEmflYYbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class State(): \n",
        "\n",
        "    def __init__(self, row=-1, column=-1):\n",
        "        self.row = row\n",
        "        self.column = column\n",
        "    \n",
        "    def clone(self):\n",
        "        return State(self.row, self.column)\n",
        "\n",
        "\n",
        "class Action(Enum): \n",
        "    UP = 1\n",
        "    DOWN = -1\n",
        "    LEFT = 2\n",
        "    RIGHT = -2\n",
        "\n",
        "\n",
        "class Environment(): \n",
        "\n",
        "    def __init__(self, grid, move_prob=0.8):      \n",
        "        # grid is 2d-array. Its values are treated as an attribute.\n",
        "        # Kinds of attribute is following.\n",
        "        #  0: ordinary cell\n",
        "        #  -1: damage cell (game end)\n",
        "        #  1: reward cell (game end)\n",
        "        #  9: block cell (can't locate agent)        \n",
        "        self.grid = grid\n",
        "        self.agent_state = State()\n",
        "        \n",
        "        # Default reward is minus. Just like a poison swamp.\n",
        "        # It means the agent has to reach the goal fast!\n",
        "        self.default_reward = -0.04\n",
        "\n",
        "        # Agent can move to a selected direction in move_prob.\n",
        "        # It means the agent will move different direction\n",
        "        # in (1 - move_prob).\n",
        "        self.move_prob = move_prob\n",
        "        self.reset()\n",
        "\n",
        "    @property\n",
        "    def row_length(self):\n",
        "        return len(self.grid)\n",
        "\n",
        "    @property\n",
        "    def column_length(self):\n",
        "        return len(self.grid[0])\n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "        return [Action.UP, Action.DOWN,\n",
        "                Action.LEFT, Action.RIGHT]\n",
        "\n",
        "    def transit_func(self, state, action):\n",
        "        transition_probs = {}\n",
        "        if not self.can_action_at(state):\n",
        "            # Already on the terminal cell.\n",
        "            return transition_probs\n",
        "\n",
        "        opposite_direction = Action(action.value * -1)\n",
        "\n",
        "        for a in self.actions:\n",
        "            prob = 0\n",
        "            if a == action:\n",
        "                prob = self.move_prob\n",
        "            elif a != opposite_direction:\n",
        "                prob = (1 - self.move_prob) / 2\n",
        "\n",
        "            next_state = self._move(state, a)\n",
        "            if next_state not in transition_probs:\n",
        "                transition_probs[next_state] = prob\n",
        "            else:\n",
        "                transition_probs[next_state] += prob\n",
        "\n",
        "        return transition_probs\n",
        "\n",
        "    def can_action_at(self, state):\n",
        "        if self.grid[state.row][state.column] == 0:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def _move(self, state, action):\n",
        "        if not self.can_action_at(state):\n",
        "            raise Exception(\"Can't move from here!\")\n",
        "\n",
        "        next_state = state.clone()\n",
        "\n",
        "        # Execute an action (move).\n",
        "        if action == Action.UP:\n",
        "            next_state.row -= 1\n",
        "        elif action == Action.DOWN:\n",
        "            next_state.row += 1\n",
        "        elif action == Action.LEFT:\n",
        "            next_state.column -= 1\n",
        "        elif action == Action.RIGHT:\n",
        "            next_state.column += 1\n",
        "\n",
        "        # Check whether a state is out of the grid.\n",
        "        if not (0 <= next_state.row < self.row_length):\n",
        "            next_state = state\n",
        "        if not (0 <= next_state.column < self.column_length):\n",
        "            next_state = state\n",
        "\n",
        "        # Check whether the agent bumped a block cell.\n",
        "        if self.grid[next_state.row][next_state.column] == 9:\n",
        "            next_state = state\n",
        "\n",
        "        return next_state\n",
        "\n",
        "    def reward_func(self, state):\n",
        "        reward = self.default_reward\n",
        "        done = False\n",
        "\n",
        "        # Check an attribute of next state.\n",
        "        attribute = self.grid[state.row][state.column]\n",
        "        if attribute == 1:\n",
        "            # Get reward! and the game ends.\n",
        "            reward = 1\n",
        "            done = True\n",
        "        elif attribute == -1:\n",
        "            # Get damage! and the game ends.\n",
        "            reward = -1\n",
        "            done = True\n",
        "\n",
        "        return reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        # Locate the agent at lower left corner.\n",
        "        self.agent_state = State(self.row_length - 1, 0)\n",
        "        return self.agent_state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, done = self.transit(self.agent_state, action)\n",
        "        if next_state is not None:\n",
        "            self.agent_state = next_state\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def transit(self, state, action):\n",
        "        transition_probs = self.transit_func(state, action)\n",
        "        if len(transition_probs) == 0:\n",
        "            return None, None, True\n",
        "\n",
        "        next_states = []\n",
        "        probs = []\n",
        "        for s in transition_probs:\n",
        "            next_states.append(s)\n",
        "            probs.append(transition_probs[s])\n",
        "\n",
        "        next_state = np.random.choice(next_states, p=probs)\n",
        "        reward, done = self.reward_func(next_state)\n",
        "        return next_state, reward, done\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q47iOedYYby",
        "colab_type": "text"
      },
      "source": [
        "<h3>プログラムの実行</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ncYuY9ZOYYby",
        "colab_type": "code",
        "outputId": "d69b639a-5f31-4f16-cc52-1d144e0edda8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import random\n",
        "\n",
        "class Agent():\n",
        "    \n",
        "    def __init__(self, env):\n",
        "        self.actions = env.actions\n",
        "        \n",
        "    def policy(self, state):\n",
        "        return random.choice(self.actions)\n",
        "    \n",
        "def main():\n",
        "    \n",
        "    # Make grid environment\n",
        "    grid = [[0,0,0,1],\n",
        "            [0,9,0,-1],\n",
        "            [0,0,0,0]]\n",
        "    \n",
        "    env = Environment(grid)\n",
        "    agent = Agent(env)\n",
        "    \n",
        "    # Try 1000 games\n",
        "    for i in range(1000):\n",
        "        # initialize position of agent\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        \n",
        "        n_step = 0\n",
        "        while not done:\n",
        "            action = agent.policy(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            n_step += 1\n",
        "        \n",
        "        if n_step == 5:\n",
        "            print(f'Episode {i:2d}: Agent gets {total_reward: 3f} reward at {n_step:3d} steps')\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 83: Agent gets -1.160000 reward at   5 steps\n",
            "Episode 350: Agent gets -1.160000 reward at   5 steps\n",
            "Episode 389: Agent gets  0.840000 reward at   5 steps\n",
            "Episode 407: Agent gets  0.840000 reward at   5 steps\n",
            "Episode 535: Agent gets -1.160000 reward at   5 steps\n",
            "Episode 537: Agent gets -1.160000 reward at   5 steps\n",
            "Episode 553: Agent gets -1.160000 reward at   5 steps\n",
            "Episode 601: Agent gets -1.160000 reward at   5 steps\n",
            "Episode 645: Agent gets -1.160000 reward at   5 steps\n",
            "Episode 792: Agent gets -1.160000 reward at   5 steps\n",
            "Episode 910: Agent gets  0.840000 reward at   5 steps\n",
            "Episode 928: Agent gets -1.160000 reward at   5 steps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgHeIe6pYYb2",
        "colab_type": "text"
      },
      "source": [
        "$5$ ステップでゴールする経路を表示すると、報酬 = $0.84$ となる経路が存在することが確かめられる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3NcuSHMYYb2",
        "colab_type": "text"
      },
      "source": [
        "<h3>参考文献</h3>\n",
        "\n",
        "<a href=\"https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-Python%E3%81%A7%E5%AD%A6%E3%81%B6%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%85%A5%E9%96%80%E3%81%8B%E3%82%89%E5%AE%9F%E8%B7%B5%E3%81%BE%E3%81%A7-KS%E6%83%85%E5%A0%B1%E7%A7%91%E5%AD%A6%E5%B0%82%E9%96%80%E6%9B%B8-%E4%B9%85%E4%BF%9D/dp/4065142989/ref=sr_1_3?__mk_ja_JP=%E3%82%AB%E3%82%BF%E3%82%AB%E3%83%8A&keywords=Python%E3%81%A7%E5%AD%A6%E3%81%B6%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92&qid=1568288711&s=gateway&sr=8-3\">久保 隆宏 『機械学習スタートアップシリーズ Pythonで学ぶ強化学習 入門から実践まで』 講談社 (2019/02/04)</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o0FTzYBYYb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}